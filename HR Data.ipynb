{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path= \"/Users/Chris/Downloads/\"\n",
    "df = pd.read_csv(data_path+'HR_comma_sep.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the high level visualizations. I always like to use sns.pairplot to get the highest level visualization of my data.\n",
    "\n",
    "The first look isn't always pretty, but it helps us get to know the distributions of the data, and helps us look for any interesting relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if column != 'left':\n",
    "        try:\n",
    "            sns.jointplot('left', column, data = df)\n",
    "            plt.title(column)\n",
    "            plt.show()\n",
    "        except:\n",
    "            sns.countplot(x=column, hue='left', data=df)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Checking out sales vs. leaving or not.\n",
    "sns.countplot(x='sales', hue='left', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Salary vs. Leaving\n",
    "# Looks like a high compensation prevents employees from leaving. A surprise to nobody.\n",
    "sns.countplot(x='salary',hue='left', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Satisfaction Level and Salary are positively correlated, but not that strongly.\n",
    "# The average still seems to be around 60% for each though\n",
    "sns.barplot(x='salary', y='satisfaction_level', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# But on average, those with a low salary are more likely to leave.\n",
    "sns.barplot(x='salary', y='left', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Similar\n",
    "sns.barplot(x='left', y='last_evaluation', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Another question, how does last_evaluation relate to\n",
    "# satisfaction_level.\n",
    "sns.jointplot(x='last_evaluation', y='satisfaction_level', data=df,\n",
    "             kind='reg')\n",
    "# While statistically significant in isolation, we still don't know very much\n",
    "# about the relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chart plots give us a couple indicators for left. The pearson correlation coefficients can give a small clue as to the relationships between each variable, and the jointplots.\n",
    "\n",
    "1. The more hours worked by an individual, the more likely the individual is to leave.\n",
    "\n",
    "2. Receiving a promotion in the last five years decreases the likelihood of leaving\n",
    "\n",
    "3. Satisfaction Level is negatively correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important question to answer in these sorts of business cases is \"What are the factors that are determining whether an employee leaves.\" We don't want to just predict whether an employee is going to leave via a black box. This is an inappropriate response to the issue. What we actually want is some way to know what factors are contributing the most to an employee leaving.\n",
    "\n",
    "<h3>Constructing a Model</h3>\n",
    "What we will need to do is deal with some of the categorical variables. We need to one-hot encode non-orderable categorical variables. For example, the sales category must have a column per sales category, since there is no order implied in the categories.  However salary should be coded as -1, 0, +1 for low, medium, high respectively.\n",
    "\n",
    "I prefer to use Pandas to do the recoding, because it is super easy, and I love dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the dummies for sales\n",
    "sales_dummies = pd.get_dummies(df['sales'])\n",
    "\n",
    "df = pd.concat([df, sales_dummies], axis=1)\n",
    "df.drop('sales', inplace = True, axis=1)\n",
    "\n",
    "def recode_sales(row):\n",
    "    if row == 'low':\n",
    "        return -1\n",
    "    elif row == 'medium':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "df['salary'] = df['salary'].apply(recode_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluating Initial Model Options</h3>\n",
    "The choice of model in this case is partially dictated by business needs. Ideally we would like some causal relationship, but we don't have the data to get that. We could do with some sort of inference, like through a properly applied Logistic Regression. Another option is a Random Forest, which gives \"Variable Importance.\" If we were to give a presentation to management (which we won't obviously), a decision tree could be an easily communicable way to management, especially if it had comparable predictive performance to the random forest.\n",
    "\n",
    "\n",
    "<h3>Logistic Regression</h3>\n",
    "Since we are dealing with a binary model, a logistic regression is a linear classifier that allows for a flexible enough fit (with basis transformations) while allowing for interpretability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.794729275275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit = LogisticRegression()\n",
    "\n",
    "x = np.array(df.drop('left', axis=1))\n",
    "y = np.array(df['left'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "x, y, train_size=0.8)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "logit_score = cross_val_score(logit, X_train, Y_train, cv=10)\n",
    "print(logit_score.mean()) # This is an embarassingly bad score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "satisfaction_level -4.06364676387\n",
      "last_evaluation 0.701241758085\n",
      "number_project -0.330955048032\n",
      "average_montly_hours 0.0046951926699\n",
      "time_spend_company 0.256482194405\n",
      "Work_accident -1.5513677389\n",
      "promotion_last_5years -1.23628774989\n",
      "salary -0.67981029325\n",
      "IT -0.0319042815875\n",
      "RandD -0.543508814657\n",
      "accounting 0.0784802786253\n",
      "hr 0.294209940887\n",
      "management -0.479169200253\n",
      "marketing 0.0543873081108\n",
      "product_mng -0.0816582471712\n",
      "support 0.136135577679\n",
      "technical 0.133632537671\n"
     ]
    }
   ],
   "source": [
    "# Let's investigate this score a little more closely\n",
    "# This might be wrong\n",
    "logit_fit = logit.fit(X_train, Y_train)\n",
    "logit_coef = logit_fit.coef_\n",
    "names_list = []\n",
    "for i in df.drop('left',axis=1).columns:\n",
    "    names_list.append(i)\n",
    "    \n",
    "\n",
    "for i in range(len(names_list)):\n",
    "    print(names_list[i], logit_coef[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.429809\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>left</td>       <th>  No. Observations:  </th>  <td> 14999</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td> 14982</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>    16</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Fri, 07 Apr 2017</td> <th>  Pseudo R-squ.:     </th>  <td>0.2169</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>13:39:51</td>     <th>  Log-Likelihood:    </th> <td> -6446.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -8232.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>  <td> 0.000</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>               <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>satisfaction_level</th>    <td>   -4.1853</td> <td>    0.092</td> <td>  -45.449</td> <td> 0.000</td> <td>   -4.366    -4.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>last_evaluation</th>       <td>    0.6501</td> <td>    0.141</td> <td>    4.600</td> <td> 0.000</td> <td>    0.373     0.927</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>number_project</th>        <td>   -0.3141</td> <td>    0.021</td> <td>  -14.838</td> <td> 0.000</td> <td>   -0.356    -0.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>average_montly_hours</th>  <td>    0.0041</td> <td>    0.000</td> <td>    8.538</td> <td> 0.000</td> <td>    0.003     0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>time_spend_company</th>    <td>    0.2565</td> <td>    0.015</td> <td>   17.350</td> <td> 0.000</td> <td>    0.228     0.285</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Work_accident</th>         <td>   -1.5352</td> <td>    0.090</td> <td>  -17.146</td> <td> 0.000</td> <td>   -1.711    -1.360</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>promotion_last_5years</th> <td>   -1.4487</td> <td>    0.258</td> <td>   -5.623</td> <td> 0.000</td> <td>   -1.954    -0.944</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>salary</th>                <td>   -0.6758</td> <td>    0.037</td> <td>  -18.056</td> <td> 0.000</td> <td>   -0.749    -0.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>IT</th>                    <td>   -0.1622</td> <td>    0.088</td> <td>   -1.852</td> <td> 0.064</td> <td>   -0.334     0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RandD</th>                 <td>   -0.5639</td> <td>    0.117</td> <td>   -4.810</td> <td> 0.000</td> <td>   -0.794    -0.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>accounting</th>            <td>    0.0118</td> <td>    0.101</td> <td>    0.117</td> <td> 0.907</td> <td>   -0.186     0.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hr</th>                    <td>    0.2597</td> <td>    0.100</td> <td>    2.608</td> <td> 0.009</td> <td>    0.065     0.455</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>management</th>            <td>   -0.5206</td> <td>    0.134</td> <td>   -3.874</td> <td> 0.000</td> <td>   -0.784    -0.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>marketing</th>             <td>   -0.0077</td> <td>    0.101</td> <td>   -0.076</td> <td> 0.939</td> <td>   -0.205     0.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>product_mng</th>           <td>   -0.1417</td> <td>    0.098</td> <td>   -1.440</td> <td> 0.150</td> <td>   -0.334     0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>support</th>               <td>    0.0635</td> <td>    0.069</td> <td>    0.926</td> <td> 0.355</td> <td>   -0.071     0.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>technical</th>             <td>    0.0822</td> <td>    0.064</td> <td>    1.282</td> <td> 0.200</td> <td>   -0.043     0.208</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                   left   No. Observations:                14999\n",
       "Model:                          Logit   Df Residuals:                    14982\n",
       "Method:                           MLE   Df Model:                           16\n",
       "Date:                Fri, 07 Apr 2017   Pseudo R-squ.:                  0.2169\n",
       "Time:                        13:39:51   Log-Likelihood:                -6446.7\n",
       "converged:                       True   LL-Null:                       -8232.3\n",
       "                                        LLR p-value:                     0.000\n",
       "=========================================================================================\n",
       "                            coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
       "-----------------------------------------------------------------------------------------\n",
       "satisfaction_level       -4.1853      0.092    -45.449      0.000        -4.366    -4.005\n",
       "last_evaluation           0.6501      0.141      4.600      0.000         0.373     0.927\n",
       "number_project           -0.3141      0.021    -14.838      0.000        -0.356    -0.273\n",
       "average_montly_hours      0.0041      0.000      8.538      0.000         0.003     0.005\n",
       "time_spend_company        0.2565      0.015     17.350      0.000         0.228     0.285\n",
       "Work_accident            -1.5352      0.090    -17.146      0.000        -1.711    -1.360\n",
       "promotion_last_5years    -1.4487      0.258     -5.623      0.000        -1.954    -0.944\n",
       "salary                   -0.6758      0.037    -18.056      0.000        -0.749    -0.602\n",
       "IT                       -0.1622      0.088     -1.852      0.064        -0.334     0.009\n",
       "RandD                    -0.5639      0.117     -4.810      0.000        -0.794    -0.334\n",
       "accounting                0.0118      0.101      0.117      0.907        -0.186     0.210\n",
       "hr                        0.2597      0.100      2.608      0.009         0.065     0.455\n",
       "management               -0.5206      0.134     -3.874      0.000        -0.784    -0.257\n",
       "marketing                -0.0077      0.101     -0.076      0.939        -0.205     0.190\n",
       "product_mng              -0.1417      0.098     -1.440      0.150        -0.334     0.051\n",
       "support                   0.0635      0.069      0.926      0.355        -0.071     0.198\n",
       "technical                 0.0822      0.064      1.282      0.200        -0.043     0.208\n",
       "=========================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now sci-kit learn isn't actually the right tool to use for this.\n",
    "# This is more for a statsmodels thing\n",
    "import statsmodels.api as sm\n",
    "sm_logit = sm.Logit(df['left'], df.drop('left',axis=1))\n",
    "sm_logit_result = sm_logit.fit()\n",
    "\n",
    "sm_logit_result.summary()\n",
    "# This gives us some information as to the significance of the variables,\n",
    "# and it seems that satisfaction level is the largest determinant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Interpreting Regression Results\n",
    "Our first stab at Logistic Regression did not produce a great prediction score through scikit learn, however we were able to examine the coefficients and their significance through sci-kit learn and statsmodels. Note that there are slight differences between the coefficient estimates between the two models, this is because sci-kit learns package uses regularization by default in their models, whereas statsmodels does not.\n",
    "\n",
    "### Bias in Logistic Regression\n",
    "\n",
    "We are running a very simple model, where each of the coefficients are linear, and of one degree. This is not particularly helpful, as one could imagine, since the complexity of the relationships in reality are likely not linear.\n",
    "\n",
    "One way to get a little more of a complex model is by employing basis expansions of our data. By adding higher-order polynomials, we can increase complexity. The tradeoff is the risk of overfitting. Combining Variable Selection with high-order basis transformations could allows us to increase the flexibility where it's warranted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's go back to the original training dataframe\n",
    "\n",
    "df_train = df.drop('left', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function that performs basis expansion of orders and interactions in your dataframe\n",
    "def basis_expansion(data_frame, order=2, interactions = False):\n",
    "    # Creates empty dataframe\n",
    "    # We will append our new columns into the df, and then\n",
    "    # concatenate our new frame to the original\n",
    "    new_df = pd.DataFrame()\n",
    "    relevant_col_list = []\n",
    "    \n",
    "    \n",
    "    def check_if_relevant(df):\n",
    "        for col in df.columns:\n",
    "            unique_vals = list(df[col].unique())\n",
    "            if not (len(unique_vals) == 2 and 0 in unique_vals and 1 in unique_vals):\n",
    "                relevant_col_list.append(col)\n",
    "            \n",
    "    check_if_relevant(data_frame)\n",
    "    for exponent in (range(1,order)):\n",
    "        exponent += 1 # For each exponent, add columns\n",
    "        for col in relevant_col_list:\n",
    "       \n",
    "            new_df[col+'**'+str(exponent)] = data_frame[col]**exponent\n",
    "    \n",
    "    if interactions:\n",
    "        for col1 in data_frame.columns:\n",
    "            for col2 in data_frame.columns:\n",
    "                if col1 != col2:\n",
    "                    new_df[col1+\"*\"+col2] = data_frame[col1]*data_frame[col2]\n",
    "    new_dataframe = pd.concat([data_frame, new_df], axis=1)\n",
    "    return new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we have a basis expansion, with likely too many terms\n",
    "df_basis_expand = basis_expansion(df_train, order=3, interactions=True)\n",
    "feature_names = [df_basis_expand.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the sklearn logistic regression\n",
    "basis_logit = LogisticRegression()\n",
    "xb_train, xb_test, yb_train, yb_test = train_test_split(np.array(df_basis_expand), np.array(df['left']), train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787814538876\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "bx_logit_score = cross_val_score(basis_logit, xb_train, yb_train, cv=10)\n",
    "print(bx_logit_score.mean()) # This is worse than before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall that sklearn's logistic regression has regularization built into it.\n",
    "\n",
    "Because of this, we can play with their regularization parameter.\n",
    "\n",
    "\n",
    "... at this point I'm pursuing Logistic Regression out of stubbornness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing out as many parameters for regularization as possible\n",
    "regularization_dict = {}\n",
    "\n",
    "for c in np.arange(0.01, 1.01, 0.03):\n",
    "    basis_logit_c = LogisticRegression(C=c, penalty='l1')\n",
    "    bx_logit_score_c = cross_val_score(basis_logit_c, xb_train, yb_train, cv=10)\n",
    "    regularization_dict[c] = bx_logit_score_c.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_val_score(LogisticRegression(C=0.001), xb_train, yb_train, cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.01: 0.90648594952727957,\n",
       " 0.040000000000000001: 0.92674539964958313,\n",
       " 0.069999999999999993: 0.94174457106104481,\n",
       " 0.099999999999999992: 0.94574519727675743,\n",
       " 0.13: 0.94741151712605354,\n",
       " 0.16: 0.94974443437113509,\n",
       " 0.19: 0.95099471226484644,\n",
       " 0.22: 0.95091144854961696,\n",
       " 0.25: 0.95099485138531337,\n",
       " 0.28000000000000003: 0.95182832360763214,\n",
       " 0.31: 0.95174505977666191,\n",
       " 0.33999999999999997: 0.95024512916328407,\n",
       " 0.37: 0.95099672517133682,\n",
       " 0.40000000000000002: 0.95257832349189131,\n",
       " 0.42999999999999999: 0.95099485115383187,\n",
       " 0.45999999999999996: 0.95007846249661743,\n",
       " 0.48999999999999999: 0.95241193460319518,\n",
       " 0.52000000000000002: 0.95266172632758772,\n",
       " 0.55000000000000004: 0.95216193471893606,\n",
       " 0.57999999999999996: 0.95307860138560285,\n",
       " 0.60999999999999999: 0.95291193471893609,\n",
       " 0.64000000000000001: 0.95232860126986196,\n",
       " 0.66999999999999993: 0.95099610069173646,\n",
       " 0.69999999999999996: 0.95157811486906119,\n",
       " 0.72999999999999998: 0.95332867088796591,\n",
       " 0.76000000000000001: 0.9532452680522695,\n",
       " 0.79000000000000004: 0.95291207372366227,\n",
       " 0.81999999999999995: 0.95349526816801033,\n",
       " 0.84999999999999998: 0.95207985057628508,\n",
       " 0.88: 0.95016082279686764,\n",
       " 0.90999999999999992: 0.95007748946353432,\n",
       " 0.93999999999999995: 0.9499981824061452,\n",
       " 0.96999999999999997: 0.95332846249661751,\n",
       " 1.0: 0.95182839299425448}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularization_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "95% on a cross-validation score is very impressive for an order two model!\n",
    "The next step is to evaluate the coefficients, and run it through statsmodel's logistic regression, to get confidence intervals. Further fine tweaking could possibly allow for interpretable coefficients. \n",
    "\n",
    "P.S the highest Cross-Validation score is with a C value of 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regularization_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-617106a36e0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mregularization_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'regularization_dict' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
